{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88dde46",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn gradio accelerate\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import gradio as gr\n",
    "from torch.utils.data import Dataset\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"Loading AG News dataset...\")\n",
    "start_time = time.time()\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "print(f\"Dataset loaded in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "id2label = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Technology\"}\n",
    "label2id = {\"World\": 0, \"Sports\": 1, \"Business\": 2, \"Technology\": 3}\n",
    "\n",
    "print(f\"Full dataset - Train: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "print(f\"Classes: {list(id2label.values())}\")\n",
    "\n",
    "\n",
    "def create_quick_subset(dataset_split, samples_per_class=300):\n",
    "    \"\"\"Create balanced subset with limited samples per class\"\"\"\n",
    "    df = pd.DataFrame(dataset_split)\n",
    "\n",
    "  \n",
    "    subset_data = []\n",
    "    for label in range(4):\n",
    "        class_data = df[df['label'] == label].sample(n=min(samples_per_class, len(df[df['label'] == label])))\n",
    "        subset_data.append(class_data)\n",
    "\n",
    "    result = pd.concat(subset_data, ignore_index=True)\n",
    "    result = result.sample(frac=1).reset_index(drop=True)  # Shuffle\n",
    "    return result\n",
    "\n",
    "print(\"Creating quick training subset...\")\n",
    "train_df = create_quick_subset(dataset['train'], samples_per_class=300)  \n",
    "test_df = create_quick_subset(dataset['test'], samples_per_class=75)     \n",
    "\n",
    "print(f\"Quick dataset - Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "print(\"Class distribution in training set:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "class QuickNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128): \n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = QuickNewsDataset(train_df['text'].tolist(), train_df['label'].tolist(), tokenizer)\n",
    "test_dataset = QuickNewsDataset(test_df['text'].tolist(), test_df['label'].tolist(), tokenizer)\n",
    "\n",
    "\n",
    "print(\"Loading DistilBERT model...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='weighted')\n",
    "\n",
    "    return {'accuracy': accuracy, 'f1': f1}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./quick_results',\n",
    "    num_train_epochs=2,              \n",
    "    per_device_train_batch_size=32,  \n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=50,                 \n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,              \n",
    "    logging_dir='./quick_logs',\n",
    "    logging_steps=20,                \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,                  \n",
    "    save_strategy=\"no\",              \n",
    "    load_best_model_at_end=False,    \n",
    "    dataloader_num_workers=2,\n",
    "    fp16=True,                       \n",
    "    gradient_checkpointing=True,     \n",
    "    max_steps=150,                   \n",
    "    disable_tqdm=False,              \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Starting QUICK training (should complete in 10-15 minutes)...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "training_start = time.time()\n",
    "trainer.train()\n",
    "training_time = time.time() - training_start\n",
    "\n",
    "print(f\"\\nTraining completed in {training_time/60:.1f} minutes!\")\n",
    "\n",
    "\n",
    "print(\"\\nQuick evaluation...\")\n",
    "eval_start = time.time()\n",
    "results = trainer.evaluate()\n",
    "eval_time = time.time() - eval_start\n",
    "\n",
    "print(f\"Evaluation completed in {eval_time:.1f} seconds\")\n",
    "print(\"\\nQuick Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = test_df['label'].tolist()\n",
    "\n",
    "print(\"\\nQuick Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=list(id2label.values())))\n",
    "\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "test_headlines = [\n",
    "    \"Apple releases new iPhone with revolutionary camera technology\",\n",
    "    \"Stock markets surge following positive economic indicators\",\n",
    "    \"World Cup final breaks viewership records globally\",\n",
    "    \"Scientists discover breakthrough in quantum computing research\",\n",
    "    \"New trade agreement signed between major economies\"\n",
    "]\n",
    "\n",
    "print(\"\\nQuick Sample Predictions:\")\n",
    "for headline in test_headlines:\n",
    "    result = classifier(headline)\n",
    "    print(f\"Text: {headline}\")\n",
    "    print(f\"Predicted: {result[0]['label']} (confidence: {result[0]['score']:.3f})\\n\")\n",
    "\n",
    "def quick_predict(text):\n",
    "    if not text.strip():\n",
    "        return \"Please enter a news headline\"\n",
    "\n",
    "    try:\n",
    "        result = classifier(text)\n",
    "        label = result[0]['label']\n",
    "        confidence = result[0]['score']\n",
    "        return f\" Category: {label}\\n Confidence: {confidence:.3f}\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "def create_quick_interface():\n",
    "    interface = gr.Interface(\n",
    "        fn=quick_predict,\n",
    "        inputs=gr.Textbox(\n",
    "            lines=2,\n",
    "            placeholder=\"Enter news headline (e.g., 'Apple announces new product launch')\",\n",
    "            label=\"News Headline\"\n",
    "        ),\n",
    "        outputs=gr.Textbox(label=\"Quick Prediction\"),\n",
    "        title=\"Quick News Topic Classifier\",\n",
    "        description=\"Fast BERT-based classifier trained on AG News dataset\\n World |  Sports |  Business | Technology\",\n",
    "        examples=[\n",
    "            \"Tesla stock rises after quarterly earnings beat expectations\",\n",
    "            \"Champions League final set for this weekend\",\n",
    "            \"New AI breakthrough announced by Google researchers\",\n",
    "            \"UN climate summit addresses global warming concerns\",\n",
    "            \"Cryptocurrency prices fluctuate amid market uncertainty\"\n",
    "        ],\n",
    "        theme=gr.themes.Soft()\n",
    "    )\n",
    "    return interface\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  Total Time: {total_time/60:.1f} minutes\")\n",
    "print(f\" Training Time: {training_time/60:.1f} minutes\")\n",
    "print(f\" Evaluation Time: {eval_time:.1f} seconds\")\n",
    "print(f\" Test Accuracy: {results['eval_accuracy']:.3f}\")\n",
    "print(f\" Test F1-Score: {results['eval_f1']:.3f}\")\n",
    "print(f\" Training Samples: {len(train_dataset)}\")\n",
    "print(f\" Test Samples: {len(test_dataset)}\")\n",
    "print(f\" Model: {model_name}\")\n",
    "print(f\" Parameters: ~66M (DistilBERT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n Launching Quick Gradio Interface...\")\n",
    "quick_interface = create_quick_interface()\n",
    "\n",
    "quick_interface.launch(\n",
    "    share=True,\n",
    "    debug=True,\n",
    "    server_name=\"0.0.0.0\",\n",
    "    show_error=True\n",
    ")\n",
    "\n",
    "print(\"\\nQuick BERT News Classifier is ready!\")\n",
    "print(\" Use the Gradio interface above to test your model\")\n",
    "print(\"Training completed in under 20 minutes!\")\n",
    "\n",
    "\n",
    "save_model = input(\"\\nSave the quick model? (y/n): \").lower().strip()\n",
    "if save_model == 'y':\n",
    "    model_path = \"./quick-bert-classifier\"\n",
    "    model.save_pretrained(model_path)\n",
    "    tokenizer.save_pretrained(model_path)\n",
    "    print(f\"Model saved to {model_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
